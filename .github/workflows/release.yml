name: 'Release on PyPi'

on:
  push:
    branches:
      - develop
    tags:
      - '*'
    paths:
      - '/src/**'
      - '/tests/**'
  workflow_dispatch:
jobs:
  tests:
    name: 'Execute unit tests'
    runs-on: ubuntu-22.04
    defaults:
      run:
        shell: bash
    steps:
    - name: Checkout
      uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12.8"
    - name: Install requirements
      run: | 
        pip install -r src/requirements.txt
        pip install pytest==8.3.3
    - name: Run MySQL container
      run: | 
        docker run --name ploosh_mysql \
          -e MYSQL_ROOT_PASSWORD="${{ secrets.TEST_LOCAL_DB_PASSWORD }}" \
          -e MYSQL_PASSWORD="${{ secrets.TEST_LOCAL_DB_PASSWORD }}" \
          -e MYSQL_DATABASE=ploosh \
          -e MYSQL_USER=ploosh \
          -p 3306:3306 \
          -d mysql
    - name: Run PostgreSQL container
      run: | 
        docker run --name ploosh_postgresql \
          -e POSTGRES_USER=ploosh \
          -e POSTGRES_PASSWORD="${{ secrets.TEST_LOCAL_DB_PASSWORD }}" \
          -e POSTGRES_DB=ploosh \
          -p 5432:5432 \
          -d postgres
    - name: Run SQL Server container
      run: | 
        docker run --name ploosh_mssql \
          -e ACCEPT_EULA="Y" \
          -e MSSQL_SA_PASSWORD="${{ secrets.TEST_LOCAL_DB_PASSWORD }}" \
          --hostname ploosh \
          -p 1433:1433 \
          -d \
          mcr.microsoft.com/mssql/server:2022-latest
    - name: Run Spark containers
      run: | 
        docker run -d --name ploosh-spark-master \
          -e SPARK_MODE=master \
          -e SPARK_MASTER_HOST=ploosh-spark-master \
          -p 7077:7077 -p 8081:8080 \
          -v $(pwd)/tests/.data:$(pwd)/tests/.data \
          --hostname ploosh-spark-master \
          bitnami/spark
          
        docker run -d --name ploosh-spark-worker \
          -e SPARK_MODE=worker \
          -e SPARK_MASTER_URL=spark://ploosh-spark-master:7077 \
          -v $(pwd)/tests/.data:$(pwd)/tests/.data \
          --link ploosh-spark-master:ploosh-spark-master \
          bitnami/spark

    - name: Feed databases
      run: | 
        sleep 30 # wait until all services are up

        mysql -h 127.0.0.1 -u ploosh -p'${{ secrets.TEST_LOCAL_DB_PASSWORD }}' < tests/.env/mysql/setup.sql

        export PGPASSWORD='${{ secrets.TEST_LOCAL_DB_PASSWORD }}';
        psql -h 127.0.0.1 -U ploosh -d ploosh -f tests/.env/postgresql/setup.sql

        /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "${{ secrets.TEST_LOCAL_DB_PASSWORD }}" -i tests/.env/mssql/setup.sql

    - name: Execute tests
      run: | 
        export TEST_DB_PASSWORD="${{ secrets.TEST_LOCAL_DB_PASSWORD }}"
        pytest  -rA ./tests
  publish:
    needs: tests
    name: 'Publish on PyPi'
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
        working-directory: src/
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12.8"
      - name: Install requirements
        run: | 
          pip install -r requirements.txt
          pip install wheel==0.44.0
          pip install twine==6.0.1
          pip install setuptools==75.1.0
      - name: Build package (full)
        run: python setup-full.py sdist bdist_wheel
      - name: Build package (core)
        run: python setup-core.py sdist bdist_wheel
      - name: Check package
        run: twine check dist/*
      - name: Publish
        run: | 
          if [[ $GITHUB_REF == refs/tags/* ]]; then
            echo "Deploying to production environment"
            twine upload --repository-url https://upload.pypi.org/legacy/ dist/* -u ${{ secrets.PYPI_USER }} -p '${{ secrets.PYPI_PASSWORD }}' --verbose

          else 
            echo "Deploying to test environment"
            twine upload --repository-url https://test.pypi.org/legacy/ dist/* -u ${{ secrets.PYPI_TEST_USER }} -p '${{ secrets.PYPI_TEST_PASSWORD }}' --verbose
          fi
